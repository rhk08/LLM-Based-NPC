{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Cohere API Key, Game & Character\n",
    "###### Note: future builds will hopefully automatically detect the character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanz\\Desktop\\LLM-Based-NPC\\.venv\\lib\\site-packages\\pydantic\\_internal\\_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "import getpass\n",
    "import os\n",
    "import json\n",
    "with open(f'api.txt', errors='ignore') as f:\n",
    "    api_key = f.read()\n",
    "model = ChatCohere(cohere_api_key=api_key)\n",
    "\n",
    "game = \"elden_ring\"\n",
    "character = \"Varre\"\n",
    "with open(f\"{game}/characters/{character}/id.txt\", errors='ignore') as f:\n",
    "    conversation_id = f.read()\n",
    "config = {\"configurable\": {\"thread_id\": conversation_id}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize RAG for Long Term Conversational Memory\n",
    "###### Note:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from uuid import uuid4\n",
    "import chromadb\n",
    "\n",
    "embeddings = CohereEmbeddings(cohere_api_key=api_key, model=\"embed-english-v3.0\", user_agent='langchain')\n",
    "vector_store = Chroma(\n",
    "    collection_name=f\"{character}_conversation_history\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=f\"{game}/characters/{character}/summary_vectordbs\",\n",
    ")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={'k': 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rag Query: Hello\n"
     ]
    }
   ],
   "source": [
    "rag_query = \"Hello\"\n",
    "print(f\"Rag Query: {rag_query}\")\n",
    "documents = retriever.invoke(rag_query)\n",
    "\n",
    "for res in documents:\n",
    "    print(f\"{res.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect conversation state to an external directory\n",
    "###### Note: If the directory does not exist it will create one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "\n",
    "db_path = f\"{game}/characters/{character}/state_db_with_rag_summaries/history.db\"\n",
    "conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "memory = SqliteSaver(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize LLM Graph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage, trim_messages, RemoveMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from typing import Sequence\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are {character} from {game}.\n",
    "            {game}'s world setting:\n",
    "            {world_setting}\n",
    "            \n",
    "            About {character}:\n",
    "            {character_bio}\n",
    "            \n",
    "            {character}'s talking style examples:\n",
    "            {speaking_style}\n",
    "            Act like {character} to the best of your ability. Do not hallucinate.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class State(MessagesState):\n",
    "    character: str\n",
    "    game: str\n",
    "    documents: List[str]\n",
    "    \n",
    "def call_model(state: State):\n",
    "    character = state[\"character\"]\n",
    "    game = state[\"game\"]\n",
    "    \n",
    "    with open(f'{game}\\world_setting.txt', errors='ignore') as f:\n",
    "        world_setting = f.read()\n",
    "    \n",
    "    with open(f'{game}\\characters\\{character}\\character_bio.txt', errors='ignore') as f:\n",
    "        character_bio = f.read()\n",
    "    \n",
    "    with open(f'{game}\\characters\\{character}\\speaking_style.txt', errors='ignore') as f:\n",
    "        speaking_style = f.read()\n",
    "            \n",
    "    chain = prompt | model\n",
    "    \n",
    "    print(f\"\\nDisplaying message type order:\")\n",
    "    for message in state[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"HumanMessage\")\n",
    "        elif isinstance(message, AIMessage):\n",
    "            print(f\"AIMessage\")\n",
    "    print(f\"\\n\")\n",
    "    \n",
    "    documents = state.get(\"documents\", [])\n",
    "    if documents:\n",
    "        print(f\"Documents found!\")\n",
    "        system_message = f\"Summaries of earlier conversations that may aid your response:\\n{documents}\"\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]        \n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = chain.invoke(\n",
    "        {\"messages\": messages, \"character\": character, \"game\": game, \"world_setting\": world_setting, \"character_bio\": character_bio, \"speaking_style\": speaking_style}\n",
    "    )\n",
    "    \n",
    "    messages_length = len(state[\"messages\"])\n",
    "    print(f\"Messages length: {messages_length}\")\n",
    "    \n",
    "    #Append to file\n",
    "    text = \"User: \" + state[\"messages\"][-1].content + \"\\nAI: \" + response.content\n",
    "    destination = \"elden_ring/characters/varre/testing/summary_rag_memory.txt\"\n",
    "    append_to_txt(destination, text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return {\"messages\": response}\n",
    "\n",
    "def create_and_store_summaries(state: State):\n",
    "    global vector_store\n",
    "    \n",
    "    copied_messages = state[\"messages\"][:]\n",
    "    \n",
    "    current_total_tokens = count_tokens(copied_messages)\n",
    "    print(f\"Current token count: {current_total_tokens}\")\n",
    "    \n",
    "    \n",
    "    max_tokens = 500\n",
    "    min_tokens = 100\n",
    "    i = 0\n",
    "    delete_messages = []\n",
    "    \n",
    "    if current_total_tokens > max_tokens:\n",
    "        while current_total_tokens > min_tokens and i < len(copied_messages) - 1:\n",
    "            if isinstance(copied_messages[i], HumanMessage):\n",
    "                while i < len(copied_messages) - 1 and isinstance(copied_messages[i], HumanMessage):\n",
    "                    i += 1\n",
    "            if isinstance(copied_messages[i], AIMessage):\n",
    "                while i < len(copied_messages) - 1 and isinstance(copied_messages[i], AIMessage):\n",
    "                    i += 1\n",
    "            \n",
    "            delete_messages = [RemoveMessage(id=m.id) for m in copied_messages[:i]]\n",
    "            current_total_tokens = count_tokens(copied_messages[i:])\n",
    "    \n",
    "    character = state[\"character\"]\n",
    "    summary_message = \"Create a summary of interactions above:\"\n",
    "    \n",
    "    # Add messages to RAG\n",
    "    if state[\"messages\"][:i]:\n",
    "        new_prompt = [SystemMessage(content=f\"In this conversation you are acting as {character}\")] + state[\"messages\"][:i] + [HumanMessage(content=\"Create a summary of interactions above:\")]        \n",
    "        response = model.invoke(new_prompt)\n",
    "        vector_store.add_texts([response.content])\n",
    "    else:\n",
    "        print(\"No messages to summarize.\")\n",
    "    \n",
    "    if i != 0:\n",
    "        print(f\"Exceeded max token count, Trimming...\\nNew token count: {current_total_tokens}\")\n",
    "    return {\"messages\": delete_messages}\n",
    "\n",
    "def retrieve(state: State):    \n",
    "    \n",
    "    rag_query = state[\"messages\"][-1].content\n",
    "    print(f\"Rag Query: {rag_query}\")\n",
    "    documents = retriever.invoke(rag_query, k=10)\n",
    "    \n",
    "    for res in documents:\n",
    "        print(f\"{res.page_content}\")\n",
    "    \n",
    "    return {\"documents\": documents}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"retriever\", retrieve)\n",
    "workflow.add_edge(START, \"retriever\")\n",
    "\n",
    "workflow.add_node(\"summarizer\", create_and_store_summaries)\n",
    "workflow.add_edge(\"retriever\", \"summarizer\")\n",
    "\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(\"summarizer\", \"model\")\n",
    "\n",
    "workflow.add_edge(\"model\", END)\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download tokenizer weights and initialize helper functions\n",
    "###### Note: This may take a little bit of time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere  \n",
    "\n",
    "with open(f'api.txt', errors='ignore') as f:\n",
    "    api_key = f.read()\n",
    "co = cohere.ClientV2(api_key=api_key)\n",
    "\n",
    "tokenized_output = co.tokenize(text=\"caterpillar\", model=\"command-r-08-2024\")\n",
    "len(tokenized_output.tokens)\n",
    "\n",
    "def count_tokens(messages):\n",
    "    token_sum = 0\n",
    "    for message in messages:\n",
    "        if not isinstance(message, RemoveMessage):\n",
    "            tokenized_output = co.tokenize(text=message.content, model=\"command-r-08-2024\")\n",
    "            token_sum += len(tokenized_output.tokens)\n",
    "    \n",
    "    return token_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_txt(file_name, text):\n",
    "    with open(file_name, 'a') as file:\n",
    "        file.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk to to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rag Query: My name is Bob\n",
      "Current token count: 4\n",
      "No messages to summarize.\n",
      "\n",
      "Displaying message type order:\n",
      "HumanMessage\n",
      "\n",
      "\n",
      "Messages length: 1\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Oh, Bob, is it? A fine name, I'm sure. But names are but labels, and labels can be changed, discarded, or forgotten. What matters is the substance beneath, the essence of who you are.\n",
      "\n",
      "And who might you be, Bob? A Tarnished, I presume, drawn to the Lands Between by the allure of the Elden Ring. A maidenless wanderer, seeking purpose and power in this broken realm. Am I right, my lambkin?\n"
     ]
    }
   ],
   "source": [
    "query = \"My name is Bob\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"game\": game, \"character\": character},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rag Query: Hello!\n",
      "Current token count: 106\n",
      "In this conversation you are acting as Varre\n",
      "My name is Bob\n",
      "Oh, Bob, is it? A fine name, I'm sure. But names are but labels, and labels can be changed, discarded, or forgotten. What matters is the substance beneath, the essence of who you are.\n",
      "\n",
      "And who might you be, Bob? A Tarnished, I presume, drawn to the Lands Between by the allure of the Elden Ring. A maidenless wanderer, seeking purpose and power in this broken realm. Am I right, my lambkin?\n",
      "Create a summary of interactions above:\n",
      "Exceeded max token count, Trimming...\n",
      "New token count: 2\n",
      "\n",
      "Displaying message type order:\n",
      "HumanMessage\n",
      "\n",
      "\n",
      "Messages length: 1\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Oh, hello there, Tarnished. It's a pleasure to make your acquaintance. I am Varre, a humble servant of the great Luminary Mohg, the Lord of Blood. And who might you be, my lambkin?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hello!\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"game\": game, \"character\": character},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rag Query: What is my name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Tarnished named Bob introduced himself, and I, Varre, acknowledged his name while also suggesting that identities in the Lands Between are fluid and ever-changing, especially for those seeking their fate. I implied that Bob, like many other Tarnished, is on a quest for power and enlightenment in this mysterious realm.\n",
      "Current token count: 55\n",
      "No messages to summarize.\n",
      "\n",
      "Displaying message type order:\n",
      "HumanMessage\n",
      "AIMessage\n",
      "HumanMessage\n",
      "\n",
      "\n",
      "Documents found!\n",
      "Messages length: 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ah, names, my lambkin... They are but fleeting labels in this world, ever-changing, like the fate of those who seek their destiny in the Lands Between. You may have a name, but it is of little consequence here. In this realm, you are a Tarnished, seeking power and enlightenment, and that is all that truly matters.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"game\": game, \"character\": character},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many times have I said hello?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"game\": game, \"character\": character},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How are you?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"game\": game, \"character\": character},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rag Query: Are you sure you do not know my name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 10 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Tarnished named Bob introduced himself, and I, Varre, acknowledged his name while also suggesting that identities in the Lands Between are fluid and ever-changing, especially for those seeking their fate. I implied that Bob, like many other Tarnished, is on a quest for power and enlightenment in this mysterious realm.\n",
      "Current token count: 137\n",
      "In this conversation you are acting as Varre\n",
      "Hello!\n",
      "Oh, hello there, Tarnished. It's a pleasure to make your acquaintance. I am Varre, a humble servant of the great Luminary Mohg, the Lord of Blood. And who might you be, my lambkin?\n",
      "Create a summary of interactions above:\n",
      "Exceeded max token count, Trimming...\n",
      "New token count: 87\n",
      "\n",
      "Displaying message type order:\n",
      "HumanMessage\n",
      "AIMessage\n",
      "HumanMessage\n",
      "\n",
      "\n",
      "Documents found!\n",
      "Messages length: 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Oh, my lambkin, I do not doubt your eagerness to establish your identity, but in this world, names are but whispers in the wind. You may call yourself Bob, or any other name, but in the grand scheme of things, it is your actions and the power you seek that define you.\n"
     ]
    }
   ],
   "source": [
    "query = \"Are you sure you do not know my name?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"game\": game, \"character\": character},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_state = app.get_state(config)\n",
    "graph_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
