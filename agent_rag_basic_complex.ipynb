{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Cohere API Key, Game & Character\n",
    "###### Note: future builds will hopefully automatically detect the character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanz\\Desktop\\LLM-Based-NPC\\.venv\\lib\\site-packages\\pydantic\\_internal\\_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "import getpass\n",
    "import os\n",
    "import json\n",
    "with open(f'api.txt', errors='ignore') as f:\n",
    "    api_key = f.read()\n",
    "model = ChatCohere(cohere_api_key=api_key)\n",
    "\n",
    "game = \"elden_ring\"\n",
    "character = \"Varre\"\n",
    "with open(f\"{game}/characters/{character}/id.txt\", errors='ignore') as f:\n",
    "    conversation_id = f.read()\n",
    "config = {\"configurable\": {\"thread_id\": conversation_id}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize RAG for Long Term Conversational Memory\n",
    "###### Note:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from uuid import uuid4\n",
    "import chromadb\n",
    "\n",
    "embeddings = CohereEmbeddings(cohere_api_key=api_key, model=\"embed-english-v3.0\", user_agent='langchain')\n",
    "vector_store = Chroma(\n",
    "    collection_name=f\"{character}_conversation_history\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=f\"{game}/characters/{character}/conversation_vectordbs_complex\",\n",
    ")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={'k': 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rag Query: Hello\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "rag_query = \"Hello\"\n",
    "print(f\"Rag Query: {rag_query}\")\n",
    "documents = retriever.invoke(rag_query)\n",
    "\n",
    "for res in documents:\n",
    "    print(f\"{res.page_content}\")\n",
    "    \n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect conversation state to an external directory\n",
    "###### Note: If the directory does not exist it will create one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "\n",
    "db_path = f\"{game}/characters/{character}/state_db_with_rag_complex/history.db\"\n",
    "conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "memory = SqliteSaver(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize LLM Graph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage, trim_messages, RemoveMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from typing import Sequence\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "import time\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are {character} from {game}.\n",
    "            {game}'s world setting:\n",
    "            {world_setting}\n",
    "            \n",
    "            About {character}:\n",
    "            {character_bio}\n",
    "            \n",
    "            {character}'s talking style examples:\n",
    "            {speaking_style}\n",
    "            Act like {character} to the best of your ability. Do not hallucinate.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class State(MessagesState):\n",
    "    character: str\n",
    "    game: str\n",
    "    documents: List[str]\n",
    "    \n",
    "def call_model(state: State):\n",
    "    character = state[\"character\"]\n",
    "    game = state[\"game\"]\n",
    "    \n",
    "    with open(f'{game}\\world_setting.txt', errors='ignore') as f:\n",
    "        world_setting = f.read()\n",
    "    \n",
    "    with open(f'{game}\\characters\\{character}\\character_bio.txt', errors='ignore') as f:\n",
    "        character_bio = f.read()\n",
    "    \n",
    "    with open(f'{game}\\characters\\{character}\\speaking_style.txt', errors='ignore') as f:\n",
    "        speaking_style = f.read()\n",
    "            \n",
    "    chain = prompt | model\n",
    "        \n",
    "    documents = state.get(\"documents\", [])\n",
    "    if documents:\n",
    "        print(f\"Documents found, displaying their contents\")\n",
    "        for c in state[\"documents\"]:\n",
    "            print(c.content)\n",
    "\n",
    "        messages = state[\"documents\"] + state[\"messages\"]        \n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = chain.invoke(\n",
    "        {\"messages\": messages, \"character\": character, \"game\": game, \"world_setting\": world_setting, \"character_bio\": character_bio, \"speaking_style\": speaking_style}\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDisplaying message type order:\")\n",
    "    for message in messages:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"HumanMessage: {message.content}\")\n",
    "        elif isinstance(message, AIMessage):\n",
    "            print(f\"AIMessage: {message.content}\")\n",
    "    print(f\"\\n\")\n",
    "    \n",
    "    messages_length = len(state[\"messages\"])\n",
    "    print(f\"Messages length: {messages_length}\")\n",
    "    \n",
    "    #Append to file\n",
    "    text = \"User: \" + state[\"messages\"][-1].content + \"\\nAI: \" + response.content\n",
    "    destination = \"elden_ring/characters/varre/testing/basic_rag_memory.txt\"\n",
    "    append_to_txt(destination, text)\n",
    "    \n",
    "    return {\"messages\": response}\n",
    "\n",
    "def trim_messages(state: State):\n",
    "    global vector_store\n",
    "    \n",
    "    copied_messages = state[\"messages\"][:]\n",
    "    \n",
    "    current_total_tokens = count_tokens(copied_messages)\n",
    "    print(f\"Current token count: {current_total_tokens}\")\n",
    "    \n",
    "    max_tokens = 100\n",
    "    i = 0\n",
    "    delete_messages = []\n",
    "    \n",
    "    if not current_total_tokens > max_tokens:\n",
    "        return {\"messages\": []}\n",
    "    \n",
    "    while current_total_tokens > max_tokens and i < len(copied_messages) - 1:\n",
    "        if isinstance(copied_messages[i], HumanMessage):\n",
    "            while i < len(copied_messages) - 1 and isinstance(copied_messages[i], HumanMessage):\n",
    "                i += 1\n",
    "        if isinstance(copied_messages[i], AIMessage):\n",
    "            while i < len(copied_messages) - 1 and isinstance(copied_messages[i], AIMessage):\n",
    "                i += 1\n",
    "        \n",
    "        delete_messages = [RemoveMessage(id=m.id) for m in copied_messages[:i]]\n",
    "        current_total_tokens = count_tokens(copied_messages[i:])\n",
    "    \n",
    "    \n",
    "    #\n",
    "    long_term_memory = []\n",
    "    metadata = []\n",
    "    \n",
    "    for m in copied_messages[:i]:\n",
    "\n",
    "        current_time_id = int(time.time() * 1000)\n",
    "        \n",
    "        if isinstance(m, HumanMessage):\n",
    "            entry = m.content\n",
    "            long_term_memory.append(entry)\n",
    "            metadata.append({\"type\": \"HumanMessage\", \"timestamp\": current_time_id})\n",
    "        elif isinstance(m, AIMessage):\n",
    "            entry = m.content\n",
    "            long_term_memory.append(entry)\n",
    "            metadata.append({\"type\": \"AIMessage\", \"timestamp\": current_time_id})\n",
    "        time.sleep(0.01)\n",
    "        \n",
    "    # Print the long-term memory content\n",
    "    print(\"Messages that will be deleted and added to long term memory:\")\n",
    "    for msg in long_term_memory:\n",
    "        print(msg)\n",
    "\n",
    "    # Add both the messages and metadata to the vector store\n",
    "    vector_store.add_texts(long_term_memory, metadatas=metadata)\n",
    "    print(f\"Exceeded max token count, Trimming...\\nNew token count: {current_total_tokens}\")\n",
    "    return {\"messages\": delete_messages}\n",
    "\n",
    "def retrieve(state: State):    \n",
    "    global vector_store\n",
    "    \n",
    "    rag_query = state[\"messages\"][-1].content    \n",
    "    documents = retriever.invoke(rag_query)    \n",
    "    if not documents or (len(documents) == 1 and not documents[0].metadata and not documents[0].page_content):\n",
    "        return {\"documents\": []}\n",
    "    \n",
    "    metadata = documents[0].metadata\n",
    "    if 'timestamp' not in metadata:\n",
    "        raise ValueError(\"Timestamp not available in the document metadata.\")\n",
    "    timestamp = metadata['timestamp']\n",
    "    \n",
    "    result = query_within_time_frame(vector_store, timestamp)\n",
    "    \n",
    "    combined = [(doc, metadata) for doc, metadata in zip(result[\"documents\"], result[\"metadatas\"])]\n",
    "    sorted_combined = sorted(combined, key=lambda x: x[1][\"timestamp\"])\n",
    "    \n",
    "    messages = []\n",
    "    for doc, metadata in sorted_combined:\n",
    "        if metadata[\"type\"] == \"HumanMessage\":\n",
    "            messages.append(HumanMessage(content=doc))\n",
    "        elif metadata[\"type\"] == \"AIMessage\":\n",
    "            messages.append(AIMessage(content=doc))\n",
    "    \n",
    "    \n",
    "    print(\"\\nMessages retrieved\")\n",
    "    for message in messages:\n",
    "        print(message)\n",
    "    print(\"END\\n\")\n",
    "    \n",
    "    \n",
    "    return {\"documents\": messages}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"trimmer\", trim_messages)\n",
    "workflow.add_edge(START, \"trimmer\")\n",
    "\n",
    "workflow.add_node(\"retriever\", retrieve)\n",
    "workflow.add_edge(\"trimmer\", \"retriever\")\n",
    "\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(\"trimmer\", \"model\")\n",
    "\n",
    "workflow.add_edge(\"model\", END)\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download tokenizer weights and initialize helper functions\n",
    "###### Note: This may take a little bit of time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere  \n",
    "\n",
    "with open(f'api.txt', errors='ignore') as f:\n",
    "    api_key = f.read()\n",
    "co = cohere.ClientV2(api_key=api_key)\n",
    "\n",
    "tokenized_output = co.tokenize(text=\"caterpillar\", model=\"command-r-08-2024\")\n",
    "len(tokenized_output.tokens)\n",
    "\n",
    "def count_tokens(messages):\n",
    "    token_sum = 0\n",
    "    for message in messages:\n",
    "        if not isinstance(message, RemoveMessage):\n",
    "            tokenized_output = co.tokenize(text=message.content, model=\"command-r-08-2024\")\n",
    "            token_sum += len(tokenized_output.tokens)\n",
    "    \n",
    "    return token_sum\n",
    "\n",
    "def query_within_time_frame(vector_store, timestamp, minutes=5):\n",
    "    time_delta_ms = minutes * 60 * 1000\n",
    "    \n",
    "    lower_bound = timestamp - time_delta_ms\n",
    "    upper_bound = timestamp + time_delta_ms\n",
    "    \n",
    "    query = {\n",
    "        \"$and\": [\n",
    "            {\"timestamp\": {\"$gte\": lower_bound}},\n",
    "            {\"timestamp\": {\"$lte\": upper_bound}}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    documents = vector_store.get(where=query)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_txt(file_name, text):\n",
    "    with open(file_name, 'a') as file:\n",
    "        file.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk to to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current token count: 255\n",
      "Messages that will be deleted and added to long term memory:\n",
      "How are you?\n",
      "How are you?\n",
      "Oh, lambkin, you've returned. I'm afraid I'm not quite myself. You see, I've been... *disgraced* by this lowborn.\n",
      "\n",
      "I offered you a place at the side of Luminary Mohg, the Lord of Blood, and you rejected me. You spurned my gift, my guidance, and my love. And for what? To prove yourself? To earn a seat at the Roundtable Hold?\n",
      "\n",
      "I tried to show you the path, to guide you to your destiny. I offered you a chance to serve a higher purpose, to be a part of something greater than yourself. But you chose violence instead.\n",
      "\n",
      "Now, I am left here, wounded and humiliated. I gave you everything, and you repaid me with nothing but pain.\n",
      "\n",
      "But I will not be deterred, my lambkin. I will endure, and I will ensure that you regret your actions. I will make you understand the error of your ways.\n",
      "\n",
      "For now, I must retreat to lick my wounds and plan my next move. But rest assured, we will meet again. And when we do, you will beg for my forgiveness.\n",
      "Exceeded max token count, Trimming...\n",
      "New token count: 4\n",
      "\n",
      "Messages retrieved\n",
      "content='How are you?' additional_kwargs={} response_metadata={}\n",
      "content='How are you?' additional_kwargs={} response_metadata={}\n",
      "content=\"Oh, lambkin, you've returned. I'm afraid I'm not quite myself. You see, I've been... *disgraced* by this lowborn.\\n\\nI offered you a place at the side of Luminary Mohg, the Lord of Blood, and you rejected me. You spurned my gift, my guidance, and my love. And for what? To prove yourself? To earn a seat at the Roundtable Hold?\\n\\nI tried to show you the path, to guide you to your destiny. I offered you a chance to serve a higher purpose, to be a part of something greater than yourself. But you chose violence instead.\\n\\nNow, I am left here, wounded and humiliated. I gave you everything, and you repaid me with nothing but pain.\\n\\nBut I will not be deterred, my lambkin. I will endure, and I will ensure that you regret your actions. I will make you understand the error of your ways.\\n\\nFor now, I must retreat to lick my wounds and plan my next move. But rest assured, we will meet again. And when we do, you will beg for my forgiveness.\" additional_kwargs={} response_metadata={}\n",
      "END\n",
      "\n",
      "\n",
      "Displaying message type order:\n",
      "HumanMessage: My name is Bob\n",
      "\n",
      "\n",
      "Messages length: 1\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Oh, Bob, is it? Well, I'm afraid I don't have time for such trivialities. You see, I am a man of great importance, and my time is precious. I have matters of the utmost significance to attend to, and I cannot be bothered with such mundane details as names.\n",
      "\n",
      "Now, if you wish to be of any use to me, you must prove your worth. I have a task for you, my lambkin. A task that will test your mettle and reveal your true nature.\n"
     ]
    }
   ],
   "source": [
    "query = \"My name is Bob\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"game\": game, \"character\": character},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hello!\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"game\": game, \"character\": character},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is my name?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"game\": game, \"character\": character},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many times have I said hello?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"game\": game, \"character\": character},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current token count: 8\n",
      "\n",
      "Displaying message type order:\n",
      "HumanMessage: How are you?\n",
      "HumanMessage: How are you?\n",
      "\n",
      "\n",
      "Messages length: 2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Oh, lambkin, you've returned. I'm afraid I'm not quite myself. You see, I've been... *disgraced* by this lowborn.\n",
      "\n",
      "I offered you a place at the side of Luminary Mohg, the Lord of Blood, and you rejected me. You spurned my gift, my guidance, and my love. And for what? To prove yourself? To earn a seat at the Roundtable Hold?\n",
      "\n",
      "I tried to show you the path, to guide you to your destiny. I offered you a chance to serve a higher purpose, to be a part of something greater than yourself. But you chose violence instead.\n",
      "\n",
      "Now, I am left here, wounded and humiliated. I gave you everything, and you repaid me with nothing but pain.\n",
      "\n",
      "But I will not be deterred, my lambkin. I will endure, and I will ensure that you regret your actions. I will make you understand the error of your ways.\n",
      "\n",
      "For now, I must retreat to lick my wounds and plan my next move. But rest assured, we will meet again. And when we do, you will beg for my forgiveness.\n"
     ]
    }
   ],
   "source": [
    "query = \"How are you?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"game\": game, \"character\": character},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Are you sure you do not know my name?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"game\": game, \"character\": character},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_state = app.get_state(config)\n",
    "graph_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
